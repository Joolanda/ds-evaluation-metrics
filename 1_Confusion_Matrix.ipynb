{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix quick Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates how to create a confusion matrix on a predicted model.\n",
    "\n",
    "\n",
    "For this, we have to import the `confusion_matrix` module from the `sklearn.metrics` library which helps us to generate the confusion matrix. We'll also have a look at the `accuracy_score` and the `classification_report`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:23:11.523092Z",
     "start_time": "2020-02-10T23:23:10.893007Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[4 2]\n",
      " [1 3]]\n",
      "Accuracy Score : 0.7\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.70      0.71      0.70        10\n",
      "weighted avg       0.72      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Within a confusion matrix, we are comparing the actual target values (y_test) with the values predicted by our model (often called y_pred)\n",
    "# Here, we are creating random values for our actual and predicted values as example\n",
    "actual    = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0] \n",
    "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0] \n",
    "\n",
    "# Here, we are creating a confusion matrix which compares actual and predicted values\n",
    "results = confusion_matrix(actual, predicted) \n",
    "\n",
    "print ('Confusion Matrix :')\n",
    "print((results) )\n",
    "\n",
    "print ('Accuracy Score :',accuracy_score(actual, predicted) )\n",
    "\n",
    "print ('Report : ')\n",
    "print (classification_report(actual, predicted) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation\n",
    "Take a moment to orient yourself so you don't mix things up:  \n",
    "- In the confusion matrix, what do rows mean and what do columns mean?  \n",
    "- Where are the TP, FP, TN, and FN? \n",
    "> Hint: They are easily identifiable by counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a generic confusion matrix:\n",
    "\n",
    "        | Predicted Positive\t            | Predicted Negative\n",
    "-------------------------------------------------------------        \n",
    "Actual  | Positive\tTrue Positive (TP)\t    |   False Negative (FN)\n",
    "Actual  | Negative\tFalse Positive (FP)\t    |   True Negative (TN)\n",
    "\n",
    "If FP = 1, but FN is high, the model misses too many true cases!\n",
    "If FN = 1, but FP is high, healthy people are repeatedly flagged.\n",
    "Best model: Minimizes both, but how much you tolerate of each depends on the application.\n",
    "\n",
    "TN: 4\n",
    "FP: 2\n",
    "FN: 1\n",
    "TP: 3\n",
    "Quick Recap on Orientation\n",
    "Rows = Actual\n",
    "Columns = Predicted\n",
    "Each cell tells you \"How many times did the model predict X when the actual value was Y?\"\n",
    "If you count:\n",
    "\n",
    "The sum of row 1 (actual 0) is 6 (4+2)\n",
    "The sum of row 2 (actual 1) is 4 (1+3)\n",
    "This matches the “support” values from our report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/confusion-matrix-machine-learning/ \n",
    "\n",
    "http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic Data Science interview question is to ask \"What is better--more false positives, or false negatives?\" \n",
    "\n",
    "\n",
    "This is a trick question designed to test your critical thinking on the topics of precision and recall. \n",
    "\n",
    "\n",
    "\n",
    "As you're probably thinking, the answer is \"It depends on the problem!\". \n",
    "\n",
    "\n",
    "\n",
    "Sometimes, our model may be focused on a problem where False Positives are much worse than False Negatives, or vice versa. For instance, detecting credit card fraud. A False Positive would be when our model flags a transaction as fraudulent, and it isn't. This results in a slightly annoyed customer. On the other hand, a False Negative might be a fraudulent transaction that the company mistakenly lets through as normal consumer behavior. In this case, the credit card company could be on the hook for reimbursing the customer for thousands of dollars because they missed the signs that the transaction was fraudulent! Although being wrong is never ideal, it makes sense that credit card companies tend to build their models to be a bit too sensitive, because having a high recall saves them more money than having a high precision score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a few minutes and see if you can think of at least 2 examples each of situations where a high precision might be preferable to high recall, and 2 examples where high recall might be preferable to high precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So, in summary, and if I understand correctly, the “best” model is not always the model with only one false positive result. The best model depends on your objective and the actual costs of each type of error. In healthcare or other sensitive sectors. Let me try to answer this question. In health cases, it is preferable to have higher precision, and in cases involving less bias, a higher recall. Likewise, you would want higher precision in a model to predict a contaminated spot in soil in contact with groundwater, and higher recall when predicting a good match between pets and new pet owners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Precision Preferred\n",
    "High Precision = When you really want to trust that a positive prediction is correct\n",
    "(You want very few false positives, even if you miss some real positives.)\n",
    "\n",
    "Examples:\n",
    "\n",
    "Identifying contaminated soil spots near groundwater\n",
    " As you mentioned! Here, a false positive might lead to unnecessary and expensive soil remediation, so you want to avoid incorrectly labeling a clean spot as contaminated.\n",
    "\n",
    "Spam email filters\n",
    " You want high precision so that when an email is marked as spam, you can trust it really is spam—otherwise, you might miss an important non-spam message (a damaging false positive).\n",
    "\n",
    "Medical diagnosis of a rare disease with dangerous treatment\n",
    " If the treatment is risky or costly, you want to be sure those labeled “disease positive” actually have it (e.g., before recommending chemotherapy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Recall Preferred\n",
    "High Recall = When catching every possible positive is more important than making a few mistakes\n",
    "(You want very few false negatives, even if some false alarms slip through.)\n",
    "\n",
    "Examples:\n",
    "\n",
    "Health screenings for contagious diseases\n",
    " Missing an infected person (false negative) could let an illness spread, so you want to catch every possible case, even if it means some healthy people are told to isolate unnecessarily.\n",
    "\n",
    "Predicting good matches between pets and new pet-holders\n",
    "If the goal is to make sure every potentially suitable pairing is considered, you'd rather have a few mismatches than overlook a great adoption opportunity.\n",
    "\n",
    "Fire alarm systems\n",
    " Better to have too many false alarms (people evacuate unnecessarily) than to miss a real fire."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
